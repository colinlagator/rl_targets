{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of DDPG in a Custom Environment\n",
    "\n",
    "- In this notebook I will go over how I implemented the deep deterministic policy gradient (DDPG) algorithm, and then trained it in a custom environment I call Target Practice. \n",
    "\n",
    "- I implemented the algorithm using Pytorch and trained it on a GPU instance in Colab.\n",
    "\n",
    "- The entire training script and environment code is located in `DDPG_training.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment: Target Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/episodes_0_49.gif\" width=\"250\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The goal in this environment is simple, given an input array that contains a target, chose a point closest to the center of the target.\n",
    "- The closer the point is to the `middle of the target`, the more reward for the agent.\n",
    "- Choosing a point that is not within the target results in a reward of `-1`\n",
    "- Each episode lasts a single step, the agent chooses a point in environment and the appropriate reward is given.\n",
    "\n",
    "\n",
    "- Observation Space\n",
    "    - The observation space is a single channel array `(X, X, 1)`\n",
    "- Action Space\n",
    "    - The action space is an array with shape `(2, )`, whose values are continuous and in `(0, X-1]`, where X is the chosen dimension of the array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent: DDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Since the action space is continous, we need to choose an agent that can produce continuous actions. DDPG does that.\n",
    "\n",
    "- I used several resources to help me implement DDPG, I will link those here\n",
    "    - https://keras.io/examples/rl/ddpg_pendulum/\n",
    "    - https://arxiv.org/pdf/1509.02971\n",
    "    - https://towardsdatascience.com/deep-deterministic-policy-gradients-explained-2d94655a9b7b\n",
    "\n",
    "- To summarize the algorithm\n",
    "    - DDPG uses an actor and critic model\n",
    "    - The actor chooses the action (XY coordinates in our case) to take given the environment (an array contained a target)\n",
    "    - The critic produces a Q-value which reflects how good an action is given a current state\n",
    "    - To train, the actor takes actions, then is updated by maximizing the Q-value returned by the critic\n",
    "    - The critic is improved by being trained on a buffer of experiences that are collected as the agent is being trained\n",
    "    - The algorithm also calls for a random process used for action exploration. \n",
    "    - In my case, I add uniformly random values to the XY coordinates chosen by the actor\n",
    "    - The random values are phased out throughout training to reduce their influence, this produces explore-eploit behavior\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Episodes 0 - 49`\n",
    "\n",
    "<img src=\"images/episodes_0_49.gif\" width=\"250\" align=\"center\">\n",
    "\n",
    "`Episodes 3000 - 3049`\n",
    "\n",
    "<img src=\"images/episodes_3000_3049.gif\" width=\"250\" align=\"center\">\n",
    "\n",
    "`Episodes 7000 - 7049`\n",
    "\n",
    "<img src=\"images/episodes_7000_7049.gif\" width=\"250\" align=\"center\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Learnings\n",
    "- Explore vs Exploit\n",
    "    - I had to create a random process that would fit well for the environment that I had created\n",
    "    - I found myself thinking about the exploration as building a good training set for the agent, to show what actions yielded the best reward\n",
    "    - Creating this was an unlock for getting the agent to train\n",
    "- Actor Output Layer\n",
    "    - Although the actor's output was continous like in a regression neural net, I found myself using the tanh activation function on the output layer, then scaling the -1 to 1 output to the environment dimensions (0-249), this also seemed to be an unlock for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I created the environment so it can be made more complex\n",
    "- Additional targets can be added, then the reward system is as follows\n",
    "    - A reward for hitting the target\n",
    "    - A reward multiplier for hitting the current biggest target\n",
    "    - As targets are hit they are removed from the environment\n",
    "- I would like to try to train an agent on a more complex version of the environment"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from skimage import draw\n",
    "import math\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_target(x, y, radius, num_rings, data):\n",
    "    for i, r in zip(range(num_rings), reversed(range(num_rings))):\n",
    "        ring_size = (r+1) * (radius/num_rings)\n",
    "        if (i % 2) != 0:\n",
    "            ring_color = (1, 1, 1)\n",
    "        else:\n",
    "            ring_color = (1, 0, 0)\n",
    "        rr, cc = draw.disk((x, y), radius=ring_size, shape=data.shape)\n",
    "        data[rr, cc, :] = ring_color\n",
    "    return data\n",
    "\n",
    "def get_new_target_data(x_dim, y_dim, min_radius, max_radius, min_rings, max_rings):\n",
    "    radius = np.random.uniform(min_radius, max_radius)\n",
    "        \n",
    "    num_rings = np.random.uniform(min_rings, max_rings)\n",
    "\n",
    "    min_x = radius\n",
    "    max_x = x_dim - radius\n",
    "    min_y = radius\n",
    "    max_y = y_dim - radius\n",
    "    x = np.random.uniform(min_x, max_x)\n",
    "    y = np.random.uniform(min_y, max_y)\n",
    "    \n",
    "    return x, y, num_rings, radius\n",
    "\n",
    "def circle_intersection(x1, y1, r1, x2, y2, r2):\n",
    "  \n",
    "    dist_centers = math.sqrt((x2-x1)**2 + (y2-y1)**2)\n",
    "    radius_sum = r1 + r2\n",
    "    \n",
    "    if dist_centers < radius_sum:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def create_scene(x_dim = 1000, y_dim = 1000, \n",
    "                 num_targets=5, min_radius=50, max_radius=100, \n",
    "                 min_rings=5, max_rings=5):\n",
    "    \"\"\"\n",
    "    Creates multiple new targets of varying radius, number of rings, and color\n",
    "    \n",
    "    Parameters:\n",
    "    num_targets - the number of targets\n",
    "    min_radius - the minimum radius of a target\n",
    "    max_radius - the maximum radius of a target\n",
    "    min_rings - the minimum number of rings on a target\n",
    "    max_rings - the maximum number of rings on a target\n",
    "    \"\"\"\n",
    "    scene_arr = np.zeros((x_dim, y_dim, 3)) + 1\n",
    "    targets_metadata = []\n",
    "    \n",
    "    for n in range(num_targets):\n",
    "        x, y, num_rings, radius = get_new_target_data(x_dim, y_dim, min_radius, max_radius, min_rings, max_rings)\n",
    "        \n",
    "        # Check for overlap with existing targets and redo target if it there is overlap        \n",
    "        while True:\n",
    "            is_overlapping = False\n",
    "            \n",
    "            # Check for overlap with existing targets\n",
    "            for t in targets_metadata:\n",
    "                targets_intersect = circle_intersection(t[0], t[1], t[2], x, y, radius)\n",
    "                if targets_intersect:\n",
    "                    is_overlapping = True\n",
    "            \n",
    "            # If there is overlap, redo the target and check again. If not, break and continue with rest of target creation\n",
    "            if is_overlapping:\n",
    "                x, y, num_rings, radius = get_new_target_data(x_dim, y_dim, min_radius, max_radius, min_rings, max_rings)    \n",
    "            else:\n",
    "                break\n",
    "                \n",
    "        scene_arr = add_target(x, y, radius=radius, num_rings=int(num_rings), data=scene_arr)\n",
    "        targets_metadata.append((x, y, radius))\n",
    "        \n",
    "    return scene_arr, targets_metadata\n",
    "\n",
    "def delete_target(x, y, radius, data):\n",
    "    rr, cc = draw.disk((x, y), radius=radius, shape=data.shape)\n",
    "    data[rr, cc, :] = (1, 1, 1)\n",
    "    return data\n",
    "\n",
    "def check_hit(action, metadata, data):\n",
    "    for i, m in enumerate(metadata):\n",
    "        x, y, radius = m\n",
    "        dist = math.sqrt((action[0] - x)**2 + (action[1] - y)**2)\n",
    "        if dist < radius:\n",
    "            reward = 1 - (dist/radius)\n",
    "            biggest_target_idx = np.argmax([m[2] for m in metadata])\n",
    "            if i == biggest_target_idx:\n",
    "                reward *= 3\n",
    "            data = delete_target(x, y, radius, data)\n",
    "            metadata.remove(m)\n",
    "            return reward, metadata, data\n",
    "    return -1, metadata, data\n",
    "\n",
    "# data, metadata = create_scene()\n",
    "\n",
    "# action = (194, 241)\n",
    "\n",
    "# check_hit(action, metadata, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Targets(gym.Env):\n",
    "    def __init__(self, x_dim = 1000, y_dim = 1000, \n",
    "                 num_targets=5, min_radius=50, max_radius=100):\n",
    "        self.num_targets = num_targets\n",
    "        self.x_dim = x_dim\n",
    "        self.y_dim = y_dim\n",
    "        self.min_radius = min_radius\n",
    "        self.max_radius = max_radius\n",
    "        \n",
    "        self.observation_space = gym.spaces.Dict(\n",
    "            {\n",
    "                \"x_pos\": gym.spaces.Box(low=0, high=x_dim, shape=(num_targets, 1), dtype=np.float32),\n",
    "                \"y_pos\": gym.spaces.Box(low=0, high=y_dim, shape=(num_targets, 1), dtype=np.float32),\n",
    "                \"sizes\": gym.spaces.Box(low=min_radius, high=max_radius, shape=(num_targets, 1), dtype=np.float32),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        self.action_space = gym.spaces.Box(low=0, high=1000, shape=(2,), dtype=np.float32)\n",
    "        \n",
    "        self.num_actions = 0\n",
    "        \n",
    "    def reset(self, seed: Optional[int] = None):\n",
    "        super().reset(seed=seed)\n",
    "        self.num_actions = 0\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        self.data, self.metadata = create_scene(x_dim=self.x_dim, \n",
    "                                                num_targets=self.num_targets, \n",
    "                                                y_dim=self.y_dim, \n",
    "                                                min_radius=self.min_radius, \n",
    "                                                max_radius=self.max_radius)\n",
    "        \n",
    "        return self._get_obs()\n",
    "    \n",
    "    def step(self, action):\n",
    "        reward, self.metadata, self.data = check_hit(action, self.metadata, self.data)\n",
    "        self.num_actions += 1\n",
    "        truncated = False\n",
    "        info = self.data\n",
    "        if (len(self.metadata) == 0) or self.num_actions >= 100:\n",
    "            terminated = True\n",
    "        else:\n",
    "            terminated = False\n",
    "        observation = self._get_obs()\n",
    "        return observation, reward, terminated, truncated, info\n",
    "        \n",
    "    def _get_obs(self):\n",
    "        # x_pos = np.array([m[0] for m in self.metadata])\n",
    "        # y_pos = np.array([m[1] for m in self.metadata])\n",
    "        # sizes = np.array([m[2] for m in self.metadata])\n",
    "        # obs_dict = {\"x_pos\": x_pos, \"y_pos\": y_pos, \"sizes\": sizes}\n",
    "        # return torch.tensor(np.hstack(list(obs_dict.values())), dtype=torch.float32)\n",
    "        return torch.tensor(self.data, dtype=torch.float32).expand(1, -1, -1, -1).permute(0, 3, 1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References for coding DDPG agent\n",
    "- https://keras.io/examples/rl/ddpg_pendulum/\n",
    "- https://arxiv.org/pdf/1509.02971\n",
    "- https://towardsdatascience.com/deep-deterministic-policy-gradients-explained-2d94655a9b7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        \n",
    "        self.state_conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.state_conv2 = nn.Conv2d(16, 32, kernel_size=10, stride=2)\n",
    "        self.state_conv3 = nn.Conv2d(32, 64, kernel_size=10, stride=2)\n",
    "        self.state_pool1 = nn.MaxPool2d(kernel_size=15, stride=2)\n",
    "        self.state_flatten1 = nn.Flatten()\n",
    "        self.state_linear1 = nn.Linear(256, 128)\n",
    "        self.state_linear2 = nn.Linear(128, 32)\n",
    "\n",
    "        self.action_linear1 = nn.Linear(action_dim, 32)\n",
    "        \n",
    "        self.linear1 = nn.Linear(64, 256)\n",
    "        self.linear2 = nn.Linear(256, 128)\n",
    "        \n",
    "        self.out = nn.Linear(128, action_dim)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        state_x = F.relu(self.state_conv1(state))\n",
    "        state_x = F.relu(self.state_conv2(state_x))\n",
    "        state_x = F.relu(self.state_conv3(state_x))\n",
    "        state_x = self.state_pool1(state_x)\n",
    "        state_x = self.state_flatten1(state_x)\n",
    "        state_x = F.relu(self.state_linear1(state_x))\n",
    "        state_x = F.relu(self.state_linear2(state_x))\n",
    "        \n",
    "        action_x = F.relu(self.action_linear1(action))\n",
    "        \n",
    "        x = torch.cat([state_x, action_x], dim=1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        \n",
    "        x = F.tanh(self.out(x))\n",
    "\n",
    "        return x\n",
    "    \n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, action_dim):\n",
    "        super(Actor, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=10, stride=2)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=10, stride=2)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=15, stride=2)\n",
    "        self.flatten1 = nn.Flatten()\n",
    "        self.linear1 = nn.Linear(256, 128)\n",
    "        self.linear2 = nn.Linear(128, 64)\n",
    "        self.linear3 = nn.Linear(64, 32)\n",
    "        \n",
    "        self.out = nn.Linear(32, action_dim)\n",
    "        #self.out.weight.data = nn.init.uniform_(self.out.weight, -0.003, 0.003)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.conv1(state))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.flatten1(x)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = F.relu(self.linear3(x))\n",
    "        x = self.out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0425,  0.1723]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_actor = Actor(2)\n",
    "temp_actor.forward(torch.rand(1, 3, 200, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1102, -0.0026]], grad_fn=<TanhBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_critic = Critic(2)\n",
    "temp_critic.forward(torch.rand(1, 3, 200, 200), torch.rand(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state, actor_model, ep, num_explore_ep, action_lower_bound, action_upper_bound):\n",
    "    actions = actor_model(state)\n",
    "    \n",
    "    if ep < num_explore_ep:\n",
    "        actions = actions + torch.tensor(np.random.uniform(action_lower_bound, action_upper_bound, actions.shape))\n",
    "    \n",
    "    bounded_action = torch.clamp(actions, action_lower_bound, action_upper_bound)\n",
    "    \n",
    "    return bounded_action[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([132.6575,  16.1274], dtype=torch.float64, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_actor = Actor(2)\n",
    "policy(torch.rand(1, 3, 200, 200), temp_actor, 0, 0, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    def __init__(self, state_dim, num_actions, buffer_capacity=100000, batch_size=64):\n",
    "        # Number of \"experiences\" to store at max\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        # Num of tuples to train on.\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Its tells num of times record() was called.\n",
    "        self.buffer_counter = 0\n",
    "\n",
    "        state_buffer_dim = [self.buffer_capacity]\n",
    "        state_buffer_dim.extend(list(state_dim))\n",
    "        \n",
    "        self.state_buffer = torch.zeros(state_buffer_dim)\n",
    "        self.action_buffer = torch.zeros((self.buffer_capacity, num_actions))\n",
    "        self.reward_buffer = torch.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = torch.zeros(state_buffer_dim)\n",
    "\n",
    "    # Takes (s,a,r,s') observation tuple as input\n",
    "    def record(self, obs_tuple):\n",
    "        # Set index to zero if buffer_capacity is exceeded,\n",
    "        # replacing old records\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "\n",
    "        self.buffer_counter += 1\n",
    "        \n",
    "    def sample(self, size):\n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        batch = np.random.choice(record_range, size)\n",
    "        \n",
    "        states = self.state_buffer[batch]\n",
    "        actions = self.action_buffer[batch]\n",
    "        rewards = self.reward_buffer[batch]\n",
    "        next_states = self.next_state_buffer[batch]\n",
    "        \n",
    "        return states, actions, rewards, next_states\n",
    "    \n",
    "def copy_network(src, target):\n",
    "    for src_param, target_param in zip(src.parameters(), target.parameters()):\n",
    "        target_param.data.copy_(src_param.data)\n",
    "\n",
    "def update_target(src, target, tau):\n",
    "    for src_param, target_param in zip(src.parameters(), target.parameters()):\n",
    "        target_param.data.copy_(tau * src_param.data + (1.0 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-91.91692679950404\n",
      "-93.87297735218881\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 58\u001b[0m\n\u001b[1;32m     56\u001b[0m actor_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mq_values\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     57\u001b[0m actor_optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 58\u001b[0m \u001b[43mactor_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m actor_optim\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     61\u001b[0m update_target(actor, target_actor, tau)\n",
      "File \u001b[0;32m~/Projects/rl_targets/venv/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/rl_targets/venv/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/rl_targets/venv/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env_dim = 200\n",
    "targets_env = Targets(x_dim=env_dim, y_dim=env_dim, num_targets=5, min_radius=10, max_radius=20)\n",
    "\n",
    "state_dim = (3, env_dim, env_dim)\n",
    "num_actions = 2\n",
    "\n",
    "actor_lr = 0.001\n",
    "actor = Actor(num_actions)\n",
    "actor_optim = torch.optim.Adam(actor.parameters(), lr=actor_lr)\n",
    "\n",
    "target_actor = Actor(num_actions)\n",
    "copy_network(actor, target_actor)\n",
    "\n",
    "critic_lr = 0.002\n",
    "critic = Critic(num_actions)\n",
    "critic_optim = torch.optim.Adam(critic.parameters(), lr=critic_lr)\n",
    "critic_criterion = nn.MSELoss()\n",
    "\n",
    "target_critic = Critic(num_actions)\n",
    "copy_network(critic, target_critic)\n",
    "\n",
    "buffer = Buffer(state_dim, num_actions, buffer_capacity=10000)\n",
    "\n",
    "num_episodes = 100\n",
    "num_explore_episodes = 20\n",
    "batch_size = 64\n",
    "gamma = 0.99\n",
    "tau = 0.005\n",
    "\n",
    "episode_rewards = []\n",
    "for ep in range(num_episodes):\n",
    "    state = targets_env.reset()\n",
    "    episode_reward = 0\n",
    "    \n",
    "    while True:\n",
    "        action = policy(state, actor, ep, num_explore_episodes, 0, env_dim)\n",
    "        next_state, reward, terminated, truncated, info = targets_env.step(action.detach().numpy())\n",
    "        buffer.record((state, action, reward, next_state))\n",
    "        episode_reward += reward\n",
    "        \n",
    "        if buffer.buffer_counter > batch_size:\n",
    "            buffer_states, buffer_actions, buffer_rewards, buffer_next_states = buffer.sample(batch_size)\n",
    "            \n",
    "            target_actions = target_actor.forward(buffer_next_states)\n",
    "            target_q_values = target_critic.forward(buffer_next_states, target_actions)\n",
    "            \n",
    "            q_prime_values = buffer_rewards + gamma * target_q_values\n",
    "            \n",
    "            q_values = critic.forward(buffer_states, buffer_actions.detach())\n",
    "            \n",
    "            critic_loss = critic_criterion(q_values, q_prime_values)\n",
    "            critic_optim.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            critic_optim.step()\n",
    "            \n",
    "            q_values = critic.forward(buffer_states, actor.forward(buffer_states))\n",
    "            actor_loss = -q_values.mean()\n",
    "            actor_optim.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_optim.step()\n",
    "            \n",
    "            update_target(actor, target_actor, tau)\n",
    "            update_target(critic, target_critic, tau)\n",
    "            \n",
    "            if terminated:\n",
    "                break\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "    episode_rewards.append(episode_reward)\n",
    "    print(episode_reward)\n",
    "    # if ep % 10 == 0:\n",
    "    #     print(f\"Episode {ep} Reward: {episode_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
